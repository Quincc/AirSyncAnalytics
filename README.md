# ğŸ“Œ Project: ETL Pipeline with Apache Airflow, PostgreSQL, and MongoDB

## ğŸ“Œ Project Description
This project is an **ETL process** implemented using **Apache Airflow**, **PostgreSQL**, and **MongoDB**. It sets up **data replication** from MongoDB to PostgreSQL and builds **analytical dashboards** to analyze user activity and support service performance.

---
## ğŸ›  Technologies Used
- **Apache Airflow** â€“ to automate ETL processes.
- **MongoDB** â€“ to store source data (NoSQL).
- **PostgreSQL** â€“ for analytics and storing structured data.
- **Python** â€“ to write DAGs for Airflow.
- **Docker (optional)** â€“ for deploying the infrastructure.

---
## ğŸ“‚ Project Structure
```
ğŸ“ airflow/
 â”œâ”€â”€ dags/
 â”‚   â”œâ”€â”€ replication_dag.py        # DAG for data replication from MongoDB to PostgreSQL
 â”‚   â”œâ”€â”€ update_analytics_dag.py   # DAG for updating analytical dashboards
ğŸ“ database/
 â”œâ”€â”€ schema_postgres.sql           # Table creation in PostgreSQL
 â”œâ”€â”€ create_showcases_postgres.sql # Analytical dashboards creation in PostgreSQL
ğŸ“ scripts/
 â”œâ”€â”€ data_generation.py            # Test data generation
ğŸ“ docs/
 â”œâ”€â”€ README.md                     # This file
```

---
## ğŸ“ Process Description

### **1ï¸âƒ£ Data Generation**
- Collections created in **MongoDB**:
  - `UserSessions` â€“ user sessions.
  - `ProductPriceHistory` â€“ price change history.
  - `EventLogs` â€“ event logs.
  - `SupportTickets` â€“ support tickets.
  - `UserRecommendations` â€“ recommendations.
  - `ModerationQueue` â€“ review moderation.
  - `SearchQueries` â€“ search queries.

ğŸ”¹ **File**:
- `data_generation.py` â€“ generates test data.

### **2ï¸âƒ£ Data Replication to PostgreSQL**
- Data from **MongoDB** is transferred to **PostgreSQL** using **Airflow DAG (`replication_dag.py`)**.
- Conversions handled:
  - `ObjectId` â†’ `string`
  - `datetime` â†’ `ISO 8601`
  - `lists` â†’ `PostgreSQL ARRAY`

ğŸ”¹ **File**: `replication_dag.py`

### **3ï¸âƒ£ Creating Analytical Dashboards**
- Dashboard **`user_activity_summary`** â€“ analyzes user activity.
- Dashboard **`support_performance`** â€“ analyzes support service performance.
- DAG **`update_analytics_dag.py`** recalculates data daily.

ğŸ”¹ **File**: `update_analytics_dag.py`
ğŸ”¹ **SQL**:
- `create_showcases_postgres.sql` â€“ script to create dashboards.

---
## ğŸš€ Project Launch
### **1ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
```

### **2ï¸âƒ£ Launch Airflow**
```bash
export AIRFLOW_HOME=~/airflow
airflow db init
airflow scheduler &
airflow webserver -p 8080 &
```

### **3ï¸âƒ£ Run Pipelines**
Open **Airflow UI** (`http://localhost:8080`):
- Run `replication_dag`
- Run `update_analytics_dag`

---
## ğŸ“Š Data Analysis
- **TOP-5 active users:**
  ```sql
  SELECT user_id, total_sessions, total_pages_visited, total_actions
  FROM user_activity_summary
  ORDER BY total_sessions DESC
  LIMIT 5;
  ```
- **Average ticket resolution time:**
  ```sql
  SELECT AVG(avg_resolution_time) AS avg_ticket_time FROM support_performance;
  ```

---
## ğŸ“Œ Evaluation Criteria
âœ… Databases deployed (MongoDB, PostgreSQL)
âœ… Replication implemented via Airflow
âœ… Data cleaned and ready for analytics
âœ… Analytical dashboards built
âœ… DAGs configured in Airflow
âœ… Process described in documentation

---
## ğŸ† Result
This project is a **full-fledged ETL pipeline** that **automates data collection, transformation, and analysis**.

ğŸš€ **Ready for deployment and scaling!**
